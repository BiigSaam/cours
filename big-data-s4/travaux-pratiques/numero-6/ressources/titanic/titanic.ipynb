{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic - Apprentissage supervisé - classification\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/fd/RMS_Titanic_3.jpg/1280px-RMS_Titanic_3.jpg\" width=\"450\" />\n",
    "\n",
    "Le RMS Titanic est un paquebot transatlantique britannique qui fait naufrage dans l'océan Atlantique Nord en 1912 à la suite d'une collision avec un iceberg, lors de son voyage inaugural de Southampton (Angleterre) à New York (Etats-Unis d'Amérique). Entre 1 490 et 1 520 personnes trouvent la mort, ce qui fait de cet événement l'une des plus grandes catastrophes maritimes survenues en temps de paix et la plus grande pour l'époque.\n",
    "- [source wikipedia](https://fr.wikipedia.org/wiki/Titanic)\n",
    "\n",
    "Le jeu de données (ou dataset) du Titanic est un peu le hello world pour l'apprentissage supervisé, le but est de trouver si les personnes présentent dans le fichier de données non-labellisées (test_data.csv) auraient survécu ou non lors du nauvrage de \"l'insubmersible\".\n",
    "Le résultat étant représenté par une donnée discrète (Survie ou non), deux classes distinctes, nous allons utiliser un algorithme de classification, voici une liste non exhaustive d'algorithmes de classification, donc d'apprentissage supervisés :\n",
    "- L'arbre décisionnel (Decision Tree)\n",
    "- Forêt d'arbres décisionnels (Random Forest)\n",
    "- Régression Logistique (Logistic Regression)\n",
    "\n",
    "Mis à part l'arbre décisionnel, il n'y pas de méthodes meilleures qu'une autre, disons plutôt qu'il y en a une plus adaptée qu'une autre."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L'arbre décisionnel (Decision Tree)\n",
    "\n",
    "Le principe de l'arbre de décision est très simple : on parcourt un ensemble de décisions et on parcourt l'arbre jusqu'à arriver à la solution.\n",
    "![schéma arbre décisionnel](../_images/decision-tree.png)\n",
    "Voici un exemple d'arbre décisionnel concernant le jeu de données du Titanic. Vous remarquerez qu'un arbre de décision est très facile à lire, même sans avoir de connaissances en informatique / mathématiques.\n",
    "\n",
    "Grâce à l'algorithme d'arbre décisionnel, nous n'avons qu'à désigner les caractéristiques qui vont servir à prendre les décisions, le nombre de noeuds (\"questions\"), et le reste sera fait tout seul. L'algorithme cherchant à trouver les questions les plus pertinentes, pour arriver à la réponse finale, dans notre cas il s'agit de la survie d'un passager du Titanic. \n",
    "\n",
    "Le problème de cette méthode, c'est quelle est très instable concernant les résultats, étant donné qu'il n'y a qu'un seul et unique test, un arbre parcouru autrement dit, notre modèle peut être fortement soumis au surapprentissage (overfitting). Ce qui fait que lors de la phase de test sur données non-labellisées, le résultat sera aléatoire. Et en ML, on essaye le plus possible de limiter la variance des résultats. C'est là qu'entre en jeu l'algorithme de forêt d'arbres décisionnels (Random Forest)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forêt d'arbres décisionnels (Random Forest)\n",
    "\n",
    "L'algorithme de forêt d'arbres décisionnels reprend le même principe que l'algorithme d'arbre décisionnel à la différence prêt que la forêt d'arbres décisionnels va multiplier le nombre d'arbres, le nombre de questions (enbranchements) resteront les mêmes, en revanche les questions posées, elles changeront aussi bien au niveau de l'ordre que leur contenu, ceci limite donc les risques de surapprentissage étant donné que notre modèle ne voit pas la même chose à chaque fois. On en déduit donc que plus il y a d'arbres dans notre forêt, moins notre modèle sera soumis au surentraînement. Néanmoins, effectuer un million d'arbres n'a pas vraiment d'intérêt.\n",
    "\n",
    "![schéma arbre décisionnel](../_images/decision-tree-vs-random-forest.png)\n",
    "\n",
    "A la fin, notre algorithme va définir un seul et unique arbre basé sur les résultats des différents arbres.\n",
    "\n",
    "- [Différence arbre décisionnel et Forêt d'arbres décisionnels - anglais](https://towardsdatascience.com/decision-trees-and-random-forests-df0c3123f991)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pour les utilisateurs de Google colab\n",
    "\n",
    "Petit apparté pour les utilisateurs de google colab. Pour utiliser la méthode `pd.read_csv()`, il faudra rajouter quelques lignes de codes supplémentaires pour pouvoir charger un fichier, les voici.\n",
    "\n",
    "\n",
    "```python\n",
    "# Première cellule jupyter\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "```\n",
    "\n",
    "```python\n",
    "# Seconde cellule jupyter\n",
    "import io\n",
    "# Très important : le nom du fichier passé en paramètre de la fonction \"uploaded\" doit avoir le même nom que le fichier que vous avez uploadé\n",
    "df = pd.read_csv(io.BytesIO(uploaded['nom-du-fichier-uploader.csv']))\n",
    "```\n",
    "\n",
    "- [Voir plus  d'informations sur le chargement de fichiers externes avec Google colab](https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Charger des fichiers distants (depuis un serveur)\n",
    "\n",
    "La classe `request` de Python permet d'effectuer des requêtes serveur, on peut l'utiliser de la façon suivante pour **télécharger** un fichier distant.\n",
    "\n",
    "```python\n",
    "request.urlretrieve (\"lien-du-fichier\", \"reference-locale-du-fichier\")\n",
    "df = pd.read_csv(\"reference-locale-du-fichier\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pratiquons !\n",
    "\n",
    "Maintenant que nous avons vu deux algorithmes de classification, nous pouvons nous essayer à l'apprentissage supervisé avec les données des passagers du RMS Titanic. Et comme d'habitude, on commence par... \n",
    "\n",
    "# à-vous-de-remplacer-le-titre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      int64\n",
       "Survived       float64\n",
       "Pclass           int64\n",
       "Name            object\n",
       "Sex             object\n",
       "Age            float64\n",
       "SibSp            int64\n",
       "Parch            int64\n",
       "Ticket          object\n",
       "Fare           float64\n",
       "Cabin           object\n",
       "Embarked        object\n",
       "WikiId         float64\n",
       "Name_wiki       object\n",
       "Age_wiki       float64\n",
       "Hometown        object\n",
       "Boarded         object\n",
       "Destination     object\n",
       "Lifeboat        object\n",
       "Body            object\n",
       "Class          float64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age              0\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "WikiId           2\n",
       "Name_wiki        2\n",
       "Age_wiki         4\n",
       "Hometown         2\n",
       "Boarded          2\n",
       "Destination      2\n",
       "Lifeboat       546\n",
       "Body           804\n",
       "Class            2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vu qu'on va manipuler des dataframes, on n'oublie pas d'importer \"pandas\" et \"numpy\"\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Ensuite on charge nos données\n",
    "# train.csv va nous servir à entraîner notre modèle\n",
    "# test_data.csv nous servira après pour valider notre modèle en utilisant des donnes non-labellisées\n",
    "entrainement_df = pd.read_csv(\"train_data.csv\")\n",
    "\n",
    "\n",
    "\n",
    "test_df = pd.read_csv(\"test_data.csv\")\n",
    "# Faire la même chose avec le fichier de test\n",
    "\n",
    "entrainement_df['Age'].fillna(entrainement_df['Age'].mean(), inplace=True)\n",
    "test_df['Age'].fillna(test_df['Age'].mean(), inplace=True)\n",
    "\n",
    "# entrainement_df['Age'] = entrainement_df['Age'].astype(int)\n",
    "#display(entrainement_df.head())\n",
    "display(entrainement_df.dtypes) \n",
    "entrainement_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explication des noms des colonnes\n",
    "Liste non-exhaustive\n",
    "- PassengerId : Identifiant unique du passager\n",
    "- Survived : 0 = Décès, 1 = En vie\n",
    "- Pclass : La classe de voyage (1 = 1ere classe, 2 = 2ème classe, 3 = 3ème classe)\n",
    "- Name : Le nom du passager\n",
    "- Sex : Le sexe du passager\n",
    "- Age : L’age du passager\n",
    "- SibSp (Siblings / Spouses) : Le nombre de frères et soeurs ou époux / épouses à bord\n",
    "- Parch (Parents / Children) : Le nombre de parents ou enfants à bord\n",
    "- Ticket : Le numéro du billet de voyage\n",
    "- Fare : Le prix du billet de voyage\n",
    "- Cabin : Le numéro de la cabine\n",
    "- Lifeboat : Identifiant du canot de sauvetage\n",
    "- Destination : Lieu de destination du Passager\n",
    "- Body : identifiant du corps\n",
    "- Embarked : Lieu d’embarquement du passager (C = Cherbourg, S = Southampton, Q = Queenstown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essayons de répondre aux questions suivantes, ceci nous permettra de mieux comprendre notre jeu de données  :\n",
    "- Combien il y avait de femmes / hommes ?\n",
    "- Quel est le ratio de femmes / hommes survivants ?\n",
    "- Quel est l'âge moyen des passagers ?\n",
    "- Quel est la femme survivante la plus âgée ? (ligne)\n",
    "- Quel est l'âge de l'homme survivant le plus jeune ? (valeur)\n",
    "\n",
    "# A vous de coder\n",
    "\n",
    "A partir du DataFrame d'entraînement (train_data.csv), répondre aux questions précédentes. N'oubliez pas la fonction `display()` pour afficher les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Il y avait 314 femmes sur le Titanic'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Combien il y avait de femmes ?\n",
    "nbr_femmes = len(entrainement_df[entrainement_df.Sex == \"female\"])\n",
    "display(f\"Il y avait {nbr_femmes} femmes sur le Titanic\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nettoyage de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre but est de trouver si les membres de notre fichier de test (test_data.csv) auraient survécu lors du naufrage du Titanic. Pour ce faire, nous allons utiliser un algorithme supervisé de classification, en l'occurence celui de la forêt d'arbres décisionnels (voir plus haut pour le principe de l'algorithme).\n",
    "\n",
    "C'est la bibliothèque scikit-learn qui va nous aider pour l'apprentissage automatique, elle est très utilisée dans le monde professionnelle et intégrée nativement à Jupyter, toutefois, il ne faut pas oublier de l'importer pour pouvoir l'utiliser. La bibliothèque étant très complexe nous allons limiter l'import qu'à l'algorithme de \"Forêt d'arbres décisionnels\" avec la ligne de code suivante `from sklearn.ensemble import RandomForestClassifier`.\n",
    "\n",
    "# A nous de coder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>...</th>\n",
       "      <th>WikiId</th>\n",
       "      <th>Name_wiki</th>\n",
       "      <th>Age_wiki</th>\n",
       "      <th>Hometown</th>\n",
       "      <th>Boarded</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Lifeboat</th>\n",
       "      <th>Body</th>\n",
       "      <th>Class</th>\n",
       "      <th>AgeBand</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>...</td>\n",
       "      <td>691.0</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>22.0</td>\n",
       "      <td>Bridgerule, Devon, England</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>Qu'Appelle Valley, Saskatchewan, Canada</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>(16.336, 32.252]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>...</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Cumings, Mrs. Florence Briggs (née Thayer)</td>\n",
       "      <td>35.0</td>\n",
       "      <td>New York, New York, US</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>New York, New York, US</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(32.252, 48.168]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>...</td>\n",
       "      <td>865.0</td>\n",
       "      <td>Heikkinen, Miss Laina</td>\n",
       "      <td>26.0</td>\n",
       "      <td>Jyväskylä, Finland</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>New York City</td>\n",
       "      <td>14?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>(16.336, 32.252]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>...</td>\n",
       "      <td>127.0</td>\n",
       "      <td>Futrelle, Mrs. Lily May (née Peel)</td>\n",
       "      <td>35.0</td>\n",
       "      <td>Scituate, Massachusetts, US</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>Scituate, Massachusetts, US</td>\n",
       "      <td>D</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(32.252, 48.168]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>...</td>\n",
       "      <td>627.0</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>35.0</td>\n",
       "      <td>Birmingham, West Midlands, England</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>New York City</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>(32.252, 48.168]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>887</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>Montvila, Rev. Juozas</td>\n",
       "      <td>male</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>211536</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>514.0</td>\n",
       "      <td>Montvila, Father Juozas</td>\n",
       "      <td>27.0</td>\n",
       "      <td>Gudinė, Lithuania [76]</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>Worcester, Massachusetts, US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>(16.336, 32.252]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Graham, Miss. Margaret Edith</td>\n",
       "      <td>female</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>112053</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>137.0</td>\n",
       "      <td>Graham, Miss Margaret Edith</td>\n",
       "      <td>19.0</td>\n",
       "      <td>Greenwich, Connecticut, US</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>Greenwich, Connecticut, US</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(16.336, 32.252]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>889</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Johnston, Miss. Catherine Helen \"Carrie\"</td>\n",
       "      <td>female</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>W./C. 6607</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>...</td>\n",
       "      <td>910.0</td>\n",
       "      <td>Johnston, Miss Catherine Nellie</td>\n",
       "      <td>7.0</td>\n",
       "      <td>Thornton Heath, London, England</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>New London, Connecticut, US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>(16.336, 32.252]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>male</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>111369</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>...</td>\n",
       "      <td>28.0</td>\n",
       "      <td>Behr, Mr. Karl Howell</td>\n",
       "      <td>26.0</td>\n",
       "      <td>New York, New York, US</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>New York, New York, US</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>(16.336, 32.252]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>Dooley, Mr. Patrick</td>\n",
       "      <td>male</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>370376</td>\n",
       "      <td>7.7500</td>\n",
       "      <td>...</td>\n",
       "      <td>783.0</td>\n",
       "      <td>Dooley, Mr. Patrick J.</td>\n",
       "      <td>43.0</td>\n",
       "      <td>Patrickswell, Limerick, Ireland</td>\n",
       "      <td>Queenstown</td>\n",
       "      <td>New York City</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>(16.336, 32.252]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived  Pclass  \\\n",
       "0              1       0.0       3   \n",
       "1              2       1.0       1   \n",
       "2              3       1.0       3   \n",
       "3              4       1.0       1   \n",
       "4              5       0.0       3   \n",
       "..           ...       ...     ...   \n",
       "886          887       0.0       2   \n",
       "887          888       1.0       1   \n",
       "888          889       0.0       3   \n",
       "889          890       1.0       1   \n",
       "890          891       0.0       3   \n",
       "\n",
       "                                                  Name     Sex        Age  \\\n",
       "0                              Braund, Mr. Owen Harris    male  22.000000   \n",
       "1    Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.000000   \n",
       "2                               Heikkinen, Miss. Laina  female  26.000000   \n",
       "3         Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.000000   \n",
       "4                             Allen, Mr. William Henry    male  35.000000   \n",
       "..                                                 ...     ...        ...   \n",
       "886                              Montvila, Rev. Juozas    male  27.000000   \n",
       "887                       Graham, Miss. Margaret Edith  female  19.000000   \n",
       "888           Johnston, Miss. Catherine Helen \"Carrie\"  female  29.699118   \n",
       "889                              Behr, Mr. Karl Howell    male  26.000000   \n",
       "890                                Dooley, Mr. Patrick    male  32.000000   \n",
       "\n",
       "     SibSp  Parch            Ticket     Fare  ... WikiId  \\\n",
       "0        1      0         A/5 21171   7.2500  ...  691.0   \n",
       "1        1      0          PC 17599  71.2833  ...   90.0   \n",
       "2        0      0  STON/O2. 3101282   7.9250  ...  865.0   \n",
       "3        1      0            113803  53.1000  ...  127.0   \n",
       "4        0      0            373450   8.0500  ...  627.0   \n",
       "..     ...    ...               ...      ...  ...    ...   \n",
       "886      0      0            211536  13.0000  ...  514.0   \n",
       "887      0      0            112053  30.0000  ...  137.0   \n",
       "888      1      2        W./C. 6607  23.4500  ...  910.0   \n",
       "889      0      0            111369  30.0000  ...   28.0   \n",
       "890      0      0            370376   7.7500  ...  783.0   \n",
       "\n",
       "                                      Name_wiki  Age_wiki  \\\n",
       "0                       Braund, Mr. Owen Harris      22.0   \n",
       "1    Cumings, Mrs. Florence Briggs (née Thayer)      35.0   \n",
       "2                         Heikkinen, Miss Laina      26.0   \n",
       "3            Futrelle, Mrs. Lily May (née Peel)      35.0   \n",
       "4                      Allen, Mr. William Henry      35.0   \n",
       "..                                          ...       ...   \n",
       "886                     Montvila, Father Juozas      27.0   \n",
       "887                 Graham, Miss Margaret Edith      19.0   \n",
       "888             Johnston, Miss Catherine Nellie       7.0   \n",
       "889                       Behr, Mr. Karl Howell      26.0   \n",
       "890                      Dooley, Mr. Patrick J.      43.0   \n",
       "\n",
       "                               Hometown      Boarded  \\\n",
       "0            Bridgerule, Devon, England  Southampton   \n",
       "1                New York, New York, US    Cherbourg   \n",
       "2                    Jyväskylä, Finland  Southampton   \n",
       "3           Scituate, Massachusetts, US  Southampton   \n",
       "4    Birmingham, West Midlands, England  Southampton   \n",
       "..                                  ...          ...   \n",
       "886              Gudinė, Lithuania [76]  Southampton   \n",
       "887          Greenwich, Connecticut, US  Southampton   \n",
       "888     Thornton Heath, London, England  Southampton   \n",
       "889              New York, New York, US    Cherbourg   \n",
       "890     Patrickswell, Limerick, Ireland   Queenstown   \n",
       "\n",
       "                                 Destination Lifeboat Body Class  \\\n",
       "0    Qu'Appelle Valley, Saskatchewan, Canada      NaN  NaN   3.0   \n",
       "1                     New York, New York, US        4  NaN   1.0   \n",
       "2                              New York City      14?  NaN   3.0   \n",
       "3                Scituate, Massachusetts, US        D  NaN   1.0   \n",
       "4                              New York City      NaN  NaN   3.0   \n",
       "..                                       ...      ...  ...   ...   \n",
       "886             Worcester, Massachusetts, US      NaN  NaN   2.0   \n",
       "887               Greenwich, Connecticut, US        3  NaN   1.0   \n",
       "888              New London, Connecticut, US      NaN  NaN   3.0   \n",
       "889                   New York, New York, US        5  NaN   1.0   \n",
       "890                            New York City      NaN  NaN   3.0   \n",
       "\n",
       "              AgeBand  \n",
       "0    (16.336, 32.252]  \n",
       "1    (32.252, 48.168]  \n",
       "2    (16.336, 32.252]  \n",
       "3    (32.252, 48.168]  \n",
       "4    (32.252, 48.168]  \n",
       "..                ...  \n",
       "886  (16.336, 32.252]  \n",
       "887  (16.336, 32.252]  \n",
       "888  (16.336, 32.252]  \n",
       "889  (16.336, 32.252]  \n",
       "890  (16.336, 32.252]  \n",
       "\n",
       "[891 rows x 22 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# On importe la classe permettant d'utiliser l'algorithme de forêt d'arbres décisionnels\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# La première chose que nous allons faire c'est désigner les features de notre modèle, \n",
    "# autrement dit, les caractéristiques de notre dataframe qui peuvent avoir une importance sur la survie d'une personne\n",
    "# Par exemple le nom d'un passager ou sa destination ne risque pas d'influencer sa survie.\n",
    "\n",
    "# Ici on met notre liste de features, \n",
    "# les paramètres qu'on estime importants quant à la survie ou non d'un passager lors du naufrage du RMS Titanic.\n",
    "liste_features = ['Pclass', 'SibSp','Parch', \"Sex\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensuite, on va catégoriser nos features, on a vu précédemment que les algorithmes de classification fonctionnent avec des données qualitatives ou quantitatives. Mais en programmation, on reste toujours plus performant avec des chiffres, \n",
    "c'est là qu'entre en jeu l'encodage des données. Le but de cette phase est de transformer en chiffres des données textuelles et pandas a une fonction pour ça, la fonction \"get_dummies()\" qui prend en paramètres nos features.\n",
    "\n",
    "![schéma one hot](../_images/one-hot.png)\n",
    "https://fr.wikipedia.org/wiki/Encodage_one-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.59\n",
      "143\n"
     ]
    }
   ],
   "source": [
    "# On applique la fonction sur les deux jeux de données\n",
    "X_entrainement = pd.get_dummies(entrainement_df[liste_features])\n",
    "X_test = pd.get_dummies(test_df[liste_features])\n",
    "\n",
    "# X_train, X_test sont donc maintenant prêtes à être utilisé par l'algorithme.\n",
    "# Il nous manque une petite chose, définir la feature de sortie, dans notre cas \"Survived\"\n",
    "# grâce à la ligne suivante\n",
    "Y_entrainement = entrainement_df['Survived']\n",
    "\n",
    "foret_decisions = RandomForestClassifier(n_estimators=100, max_depth=5, random_state = 1)\n",
    "foret_decisions.fit(X_entrainement, Y_entrainement)\n",
    "modele_predictions = foret_decisions.predict(X_test)\n",
    "\n",
    "acc_random_forest = round(foret_decisions.score(X_entrainement, Y_entrainement) * 100, 2)\n",
    "print(acc_random_forest)\n",
    "\n",
    "predictions_df = pd.DataFrame({\"Name\" : test_df.Name, \"PassengerId\" : test_df.PassengerId, \"Survived\": predictions})\n",
    "print(len(predictions_df[predictions_df.Survived == 1.0]))\n",
    "# predictions_df.to_csv(\"hello.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>637</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>873</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>536</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>712 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass  SibSp  Parch  Sex_female  Sex_male\n",
       "637       2      1      1           0         1\n",
       "22        3      0      0           1         0\n",
       "565       3      2      0           0         1\n",
       "873       3      0      0           0         1\n",
       "203       3      0      0           0         1\n",
       "..      ...    ...    ...         ...       ...\n",
       "267       3      1      0           0         1\n",
       "279       3      1      1           1         0\n",
       "549       2      1      1           0         1\n",
       "536       1      0      0           0         1\n",
       "420       3      0      0           0         1\n",
       "\n",
       "[712 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>360</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>537</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>605</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>766</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Pclass  SibSp  Parch  Sex_female  Sex_male\n",
       "436       3      2      2           1         0\n",
       "45        3      0      0           0         1\n",
       "688       3      0      0           0         1\n",
       "360       3      1      4           0         1\n",
       "537       1      0      0           1         0\n",
       "..      ...    ...    ...         ...       ...\n",
       "605       3      1      0           0         1\n",
       "15        2      0      0           1         0\n",
       "77        3      0      0           0         1\n",
       "766       1      0      0           0         1\n",
       "703       3      0      0           0         1\n",
       "\n",
       "[179 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "637    0.0\n",
       "22     1.0\n",
       "565    0.0\n",
       "873    0.0\n",
       "203    0.0\n",
       "      ... \n",
       "267    1.0\n",
       "279    1.0\n",
       "549    1.0\n",
       "536    0.0\n",
       "420    0.0\n",
       "Name: Survived, Length: 712, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "436    0.0\n",
       "45     0.0\n",
       "688    0.0\n",
       "360    0.0\n",
       "537    1.0\n",
       "      ... \n",
       "605    0.0\n",
       "15     1.0\n",
       "77     0.0\n",
       "766    0.0\n",
       "703    0.0\n",
       "Name: Survived, Length: 179, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(pd.get_dummies(entrainement_df[liste_features]), Y_entrainement, test_size=0.2)\n",
    "\n",
    "display(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78230337, 0.7826087 , 0.78681627, 0.79523142, 0.78681627])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "k = np.arange(1, 2)\n",
    "train_score, val_score = validation_curve(RandomForestClassifier(), X_entrainement, Y_entrainement, param_range = k, param_name = \"max_depth\", cv=5)\n",
    "\n",
    "display(train_score.flatten())\n",
    "\n",
    "data = pd.DataFrame(np.array([train_score.flatten()]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ble.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-94-0e618a5b3e41>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlretrieve\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/Parcours_data_scientist/decouvrez-les-librairies-python-pour-la-data-science/hubble_data.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"be.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mhubble\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ble.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ble.csv'"
     ]
    }
   ],
   "source": [
    "from urllib import request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "\n",
    "request.urlretrieve (\"https://s3-eu-west-1.amazonaws.com/static.oc-static.com/prod/courses/files/Parcours_data_scientist/decouvrez-les-librairies-python-pour-la-data-science/hubble_data.csv\", \"be.csv\")\n",
    "hubble = pd.read_csv(\"ble.csv\")\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "X = hubble.distance.values.reshape(-1,1)\n",
    "Y = hubble.recession_velocity\n",
    "lr = LinearRegression()\n",
    "lr.fit(X, Y)\n",
    "\n",
    "Y_test = np.array([.032, 1.5, 2.0, .6]).reshape(-1, 1)\n",
    "y_predict = lr.predict(Y_test)\n",
    "\n",
    "display(np.array([15, 20, 60]))\n",
    "display(np.array([15, 20, 60]).reshape(-1, 1))\n",
    "\n",
    "print(lr.coef_) # a de ax + b\n",
    "print(lr.intercept_) # b de ax +b \n",
    "\n",
    "display(type(Y_test))\n",
    "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)\n",
    "\n",
    "\n",
    "\n",
    "BIGGER_SIZE = 15\n",
    "plt.rc('font', size=BIGGER_SIZE) # taille de texte par défaut\n",
    "plt.rc('axes', titlesize=BIGGER_SIZE) # taille des titres des axes\n",
    "plt.rc('axes', labelsize=BIGGER_SIZE) # taille des labels des axes\n",
    "plt.rc('xtick', labelsize=BIGGER_SIZE) # taille des ticks des ascisses\n",
    "\n",
    "plt.rc('ytick', labelsize=BIGGER_SIZE) # taille des ticks des ordonnées\n",
    "plt.rc('legend', fontsize=BIGGER_SIZE) # taille de la légende\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE) # taille du titre\n",
    "plt.figure()\n",
    "plt.title(\"Données de Hubble\")\n",
    "plt.scatter(X, Y, color='black', label=\"données\")\n",
    "plt.plot(X, lr.predict(X), color='red', linewidth=3, label=\"prédiction\")\n",
    "plt.xlabel(\"Distance\")\n",
    "plt.ylabel(\"Vitesse de récession\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "display(Y_test)\n",
    "display(y_predict)\n",
    "df = pd.DataFrame({'Actual': Y_test.flatten(), 'Predicted': y_predict})\n",
    "df\n",
    "\n",
    "#pd.DataFrame({\n",
    " #   \"Feature\":hubble.columns.tolist(), \"Coefficients\" :lr.coef_[0]\n",
    "#})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
