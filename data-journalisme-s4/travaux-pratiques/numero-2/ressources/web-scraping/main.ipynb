{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeautifulSoup\r\n",
    "\r\n",
    "BeautifulSoup est un bibliothèque de Python permettant de parser les documents HTML et XML, ainsi, il est possible d'extraire le contenu de page web selon des sélecteurs. La syntaxe des sélecteurs est la même que celle utilisée en javascript/jQuery/CSS.\r\n",
    "\r\n",
    "- Sélectionner une classe : \".nom-de-classe\"\r\n",
    "  - `soup.select(\".nom-de-classe\")` \r\n",
    "- Sélectionner une classe contenue dans une autre : \".classe-1 .classe-2\"\r\n",
    "  - `soup.select(\".classe-1 .classe-2\")` \r\n",
    "- Sélectionner un élement par son attribut : \"[src]\"\r\n",
    "  - `soup.select(\"[src]\")` \r\n",
    "\r\n",
    "Notez un point très important : BeautifulSoup n'est pas très apté pour les scraper les données asynchrones, autrement dit, si votre site cible ajoute du contenu **après** le chargement initial de la page, il est fort probable que vous n'arrivez pas à le récupérer.\r\n",
    "\r\n",
    "BeautifulSoup est une librairie externe, première étape : l'installation et l'importation.\r\n",
    "\r\n",
    "- [Voir documentation en anglais de BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pensez à commenter cette ligne après avoir importé beautifulsoup4 \r\n",
    "# pour éviter que pip vérifie s'il doit importer l'outil à chaque fois\r\n",
    "\r\n",
    "# pip install beautifulsoup4\r\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parfait, BeautifulSoup est présent, mais ce n'est pas assez pour scraper nos pages web, il nous désormais charger nos pages, ici, nous n'aurons pas besoin d'installer un outil externe. Le langage Python possède un très grand nombre de bibliothèques par défaut. C'est \"request\" qui va nous servir à charger nos pages web, même si c'est natif à Python, il faut **impérativement** l'importer avant de l'utiliser avec la ligne suivante :\r\n",
    "```import requests```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour ne pas trop complexifier nos premiers pas avec BeautifulSoup, nous allons utiliser l'outil sans faire appel à des pages externes et utiliser la bibliothèque requests. Notre outil de page de test va être le fichier \"index.html\" présent dans le dossier de la ressource.\r\n",
    "\r\n",
    "Première étape, charger notre fichier \"index.html\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemin_du_fichier = \"index.html\"\r\n",
    "\r\n",
    "with open(chemin_du_fichier) as page_web:\r\n",
    "    soup = BeautifulSoup(page_web, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voilà, notre fichier html est chargé et le contenu est chargé, sous forme d'objet, dans une variable nommée **soup**. Assurons-nous que tout fonctionne, pour ce faire, récupérons le titre de notre page (le contenu de la balise <title>) et affichons-le grâce à la fonction \"print\" ou \"display\". Notre variable **\"soup\"** étant un objet, notre balise <title> est une propriété de **\"soup\"** nommée **title**.\r\n",
    "\r\n",
    "Affichez donc la balise <title> dans la cellule ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichez ici la balise <title>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Génial, ça fonctionne, en revanche, nous avons le contenu de votre balise title, mais entouré des balises. Pour n'obtenir que le texte d'une balise, il suffit de rajouter d'accéder à la propriété \".text\" de \"soup.title\" et là, nous n'obtiendrons que le texte. \r\n",
    "\r\n",
    "Prenez tout de fois en compte le fait que la propriété \"text\" affiche tout le texte de façon transverse, ainsi si vous accèdez à \".text\" au niveau de la balise \"body\" vous allez afficher tout le contenu textuel de la page web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichez ici le contenu textuel de la balise <title>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Désormais, on sait comment afficher le contenu textuel d'une balise. Allons un peu plus loin, essayons de récupérer une balise, disons la balise <p> contenu dans la page. Même principe que précédemment, nous allons utiliser la variable **soup** mais cette fois-ci la propriété \".p\" car nous cherchons à récupérer la balise <p>. Essayons !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichez ici la balise <p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que remarquez-vous comparé à la structure de page index.html au niveau de la balise <p> ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La propriété \".p\", c'est très bien. Toutefois, s'il y a plusieurs éléments, ça montre ses limites, la propriété \".p\" (ou tout autre balise), **ça ne retourne que la première occurence**, si on veut tout retourner, il faut utiliser la méthode \".select()\" ou \".find_all()\". Ces méthodes prennent en paramètre un sélecteur CSS/javascript/jQuery.\r\n",
    "\r\n",
    "Pour commencer, on va faire simple : on va récupérer **toutes** nos balises <p> avec la méthode \".select()\" ou \".find_all()\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichez ici les balises <p> de la page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que le résultat est dans une liste, on ne pourra pas accéder directement la propriété \".text\" par exmeple, vu que ce n'est pas une propriété de la liste, il faudra passer par une boucle pour ce faire. Essayez, complétez le code suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for paragraphe in {votre liste contenant les paragraphes}:\r\n",
    "     # le texte de chaque paragraphe\r\n",
    "     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \".select()\" ou \".find_all()\" ?\r\n",
    "Les deux font la même chose, prenez juste en compte le fait que \".select()\" est plus souple et puissant que \".find_all()\". \".select()\" permet notamment l'utilisation de sélecteurs plus complexes comme les combinateurs. Par exemple `soup.select(\".paragraphe .txt-gras\")` écrire la même chose avec \".find_all()\" nécessiterait une variable pour stocker tous les \".paragraphe\" et un ensuite faire un \".find_all()\" sur \".txt-gras\". Le tout avec une syntaxe plus lourde comme suivi `soup.find_all(\"p\", {\"class\": \"paragraphe\"})`\r\n",
    "\r\n",
    "La méthode \".find_all()\" existe également en version \".find()\" qui retourne le premier résultat trouvé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"txt-gras\">\n",
       "             dans le but de le transformer pour permettre son utilisation dans un autre contexte, cette\n",
       "             technique peut Ãªtre trÃ¨s utile pour les data-journalistes.\n",
       "         </span>,\n",
       " <span class=\"txt-gras\">trÃ¨s</span>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# La méthode .select() nous donne plus de possibilités en terme de sélecteurs. \r\n",
    "# Ces électeurs sont identiques à ceux utilisés en javascript/CSS/jQuery.\r\n",
    "soup.select(\".paragraphe .txt-gras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant à vous de jouer. Scrapez le site pour récupérer les données suivantes :\r\n",
    "- La valeur de l'attribut \"src\" des balises img contenue dans les articles\r\n",
    "- Le texte des entrées de la navigation\r\n",
    "- Le texte contenu dans le classe \"txt-gras\"\r\n",
    "\r\n",
    "Ces données doivent être placée dans des variables. Pour le premier cas, il faudra utiliser un tableau pour ajouter progressivement les données.\r\n",
    "\r\n",
    "- [Voir rappel sur les tableaux en Python](https://www.w3schools.com/python/python_arrays.asp)\r\n",
    "\r\n",
    "Dernier point : pour récuperer la valeur d'un attribut, il faut récupérer l'objet HTML et mettre entre crochets le nom de l'attribut dont on souhaite récupérer la valeur. Exemple : \r\n",
    "```soup.find(\"img\")[\"src\"]\r\n",
    "```\r\n",
    "**BeautifulSoup lèvera une erreur si vous écrivez entre les crochets un attribut qui n'existe pas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Faites la pratique ici."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super, on sait maintenant récupérer des données et les stocker, mais il nous manque quelque chose. Rappelons-nous un des objectifs du web-scraping \"Transformer des données de sites web en données structurées\". Jusqu'à présent, nous avons extrait des données mais nous ne les avons pas structurées. Autrement dit, nous ne pouvons pas faire grand-chose de nos extractions jusqu'à présent. Changeons ça.\r\n",
    "\r\n",
    "Pour structurer nos données, il faut que nos données soient sous la forme d'un structure ordonnées, il y a un modèle de données tout trouvé : le tableau de dictionnaires. Une fois transformé, ça ressemble à un fichier fait sous un tableur avec des colonnes prédéfinies.\r\n",
    "\r\n",
    "Pour rappel, un dictionnaire (appelé aussi \"tableau associatif\" ou \"objet\") est une structure où les index ne sont plus des nombres mais une chaîne de caractères. C'est à dire qu'un dictionnaire est un ensemble de clé:valeur, ça ressemble à ceci :\r\n",
    "```\r\n",
    "un_dictionnaire = {\r\n",
    "  \"marque\": \"Dell\",\r\n",
    "  \"modele\": \"XPS\",\r\n",
    "  \"annee\": 2014\r\n",
    "}\r\n",
    "```\r\n",
    "\r\n",
    "- [Voir rappel sur les tableaux en Python](https://www.w3schools.com/python/python_dictionaries.asp)\r\n",
    "\r\n",
    "\r\n",
    "Et ces dictionnaires, nous pouvons les mettre dans un tableau et donc réaliser un tableau de dictionnaires soit notre donnée structurée. Essayons de créer un tableau de dictionnaires avec la liste d'articles (\".article\") présente sur notre fichier index.html. Nos dictionnaires doivent contenir les clés suivantes \r\n",
    "- auteur : auteur de l'article\r\n",
    "- image : source de l'image\r\n",
    "- title : titre de l'article\r\n",
    "- date : date de l'article\r\n",
    "\r\n",
    "Encore une fois, il faudra stocker le tout dans une variable. N'oubliez pas qu'on veut un tableau, il faudra impérativement faire une boucle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Faites la pratique ici."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On n'a notre donnée structurée, il nous manque une chose : un fichier exploitable. Et on va réutiliser pandas pour ça. Première étape : l'importation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a importé pandas, avant de créer un fichier csv, il nous faut un DataFrame sinon, ça ne fonctionnera pas pour créer un fichier. Pour créer un DataFrame, il nous suffit juste de placer notre dictionnaire dans la méthode \".DataFrame()\" de pandas et ensuite sauvegarder le tout grâce à la méthode \".to_csv()\".\r\n",
    "\r\n",
    "Complétez le code suivant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({votre tableau de dictionnaire})\r\n",
    "\r\n",
    "df.to_csv(\"chemin-de-votre-fichier.{csv}\")\r\n",
    "# df.to_excel() pour créer un fichier excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "MissingSchema",
     "evalue": "Invalid URL 'index.html': No schema supplied. Perhaps you meant http://index.html?",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMissingSchema\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5f6a7d2a04ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mchemin_du_fichier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"index.html\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mresponse\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchemin_du_fichier\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'html.parser'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(url, params, **kwargs)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'allow_redirects'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'get'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\api.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(method, url, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[1;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[0;32m    514\u001b[0m             \u001b[0mhooks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mhooks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m         )\n\u001b[1;32m--> 516\u001b[1;33m         \u001b[0mprep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_request\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m         \u001b[0mproxies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mproxies\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\sessions.py\u001b[0m in \u001b[0;36mprepare_request\u001b[1;34m(self, request)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    448\u001b[0m         \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPreparedRequest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 449\u001b[1;33m         p.prepare(\n\u001b[0m\u001b[0;32m    450\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m             \u001b[0murl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mprepare\u001b[1;34m(self, method, url, headers, files, data, params, auth, cookies, hooks, json)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_url\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_headers\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheaders\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprepare_cookies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcookies\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\requests\\models.py\u001b[0m in \u001b[0;36mprepare_url\u001b[1;34m(self, url, params)\u001b[0m\n\u001b[0;32m    386\u001b[0m             \u001b[0merror\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0merror\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mto_native_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 388\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mMissingSchema\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    389\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mhost\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMissingSchema\u001b[0m: Invalid URL 'index.html': No schema supplied. Perhaps you meant http://index.html?"
     ]
    }
   ],
   "source": [
    "import requests\r\n",
    "\r\n",
    "chemin_du_fichier = \"index.html\"\r\n",
    "\r\n",
    "response = requests.get(chemin_du_fichier)\r\n",
    "soup = BeautifulSoup(response.text, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c469c999aae2bf6d578d6881606ced546d1546d312a4811937df5b24665ee8bc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}