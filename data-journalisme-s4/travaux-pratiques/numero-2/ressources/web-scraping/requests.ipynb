{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans la première partie de la pratique, nous avons vu comment scraper une page web,toutefois, dans le cadre de notre discipline, nous n'aurons pas les pages localement, elles seront sur des serveurs distants, il faudra donc faire des requêtes pour récupérer le contenu de ces pages.\r\n",
    "\r\n",
    "Malgré ses nombreuses fonctionnalités ([voir la documentation de BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/)) BeautifulSoup ne permet pas de faire une requête vers une ressource distante. Pour ceci, nous n'aurons pas besoin d'installer un outil externe. Le langage Python possède un très grand nombre de bibliothèques par défaut. C'est \"request\" qui va nous servir à charger nos pages web, même si c'est natif à Python, il faut **impérativement** l'importer avant de l'utiliser avec la ligne suivante :\r\n",
    "```import requests```\r\n",
    "\r\n",
    "Utilisez donc la cellule suivante pour importer \"requests\" et \"BeautifulSoup\" (reprenez la ligne d'importation du fichier \"main.ipynb\" pour \"BeautifulSoup\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importez \"requests\" et \"BeautifulSoup\" ici"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parfait \"requests\" et \"BeautifulSoup\" sont importés, nous pouvons commencer à scraper. Nous allons mettre tout ça en application sur des produits du site boulanger.fr, imaginons que nous souhations récupérer certains produits, disons les casques audio et récupérer les informations suivantes :\r\n",
    "- Marque\r\n",
    "- Nom du produit\r\n",
    "- Prix\r\n",
    "- Est-ce que le produit possède un rabais ?\r\n",
    "- Le rabais du produit ?\r\n",
    "\r\n",
    " Voici l'url qui nous intéresse \"https://www.boulanger.com/c/tous-les-casques-audio?numPage=1\", si vous accédez à la page, vous verrez ceci :\r\n",
    "![](boulanger-audio.png)\r\n",
    "\r\n",
    "Nous allons commencer avec une page pour nous assurer que notre code fonctionne correctement. Mais avant, il faut récupérer nos sélecteurs grâce à la console du navigateur (onglet \"Elements/Inspecteur\"). Complètez la liste suivante avec les sélecteurs associés :\r\n",
    "\r\n",
    "- Marque :\r\n",
    "- Nom du produit :\r\n",
    "- Prix : \r\n",
    "- Est-ce que le produit possède un rabais ? : \r\n",
    "- Le rabais du produit ? :\r\n",
    "\r\n",
    "Pensez à valider vos sélecteurs via la console du navigateur (onglet \"console\") grâce à la méthode \"document.querySelector(\"votre-selecteur\")\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voilà, nous avons nos sélecteurs, il est désormais temps de charger notre page avec \"requests\" avec le code suivant. Nous allons donc faire la même chose que précédemment sauf que cette fois-ci nous allons stocker les résultats de la première page dans un tableau de dictionnaires et sauvegarder le tout dans un fichier (csv ou xls(x) au choix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\r\n",
    "import requests\r\n",
    "\r\n",
    "# Vous pouvez choisir un autre URL\r\n",
    "lien = \"https://www.boulanger.com/c/tous-les-casques-audio?numPage=1\"\r\n",
    "response = requests.get(lien)\r\n",
    "\r\n",
    "soup = BeautifulSoup(response.text, 'html.parser')\r\n",
    "\r\n",
    "# completez le code pour récupérer les données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notre code est fonctionnel, maintenant nous pouvons scraper à plus haute échelle et donc récupérer toutes les données sur toutes les pages de notre catégorie. Il nous faudra donc faire une boucle pour changer de page à la fin de chaque scrap. Si on regarde l'URL qu'il y a le paramètre \"numPage\" qui semble indiquer le numéro de page, il faudra donc mettre à jour le paramètre entre chaque tour de boucle.\r\n",
    "\r\n",
    "Cette action étant longue, mais effectuée rapidement par notre robot, il est fort probable que nos actions soient considérées comme étant suspectes par les serveurs du site et que notre adresse IP soit bannie au bout d'un moment.\r\n",
    "\r\n",
    "Pour éviter ceci, nous allons mettre en place un délai entre chaque tour, nous allons utiliser les bibliotèques \"time\" et \"random\", ces bibliothèques sont également natives à Python, il n'y aura pas besoin d'utiliser pip en amont. Le code suivant permet d'instaurer une pause dont le temps est situé entre 15 et 35 secondes `time.sleep(random.randrange(15, 35))`, il faudra mettre ce code au début de sa première boucle.\r\n",
    "\r\n",
    "Pour réaliser le scrap complet, nous aurons besoin, au minimum, de deux boucles :\r\n",
    "- Une boucle permettant d'itérer entre les pages (c'est là qu'on trouvera `time.sleep(random.randrange(15, 35))`)\r\n",
    "- Une boucle itérant entre les produits de la page courante\r\n",
    "\r\n",
    "- [Voir documentation des boucles en Python](https://www.w3schools.com/python/python_for_loops.asp)\r\n",
    "- [Voir documentation sur les tableaux en Python](https://www.w3schools.com/python/python_arrays.asp)\r\n",
    "- [Voir rappel sur les dictionnaires en Python](https://www.w3schools.com/python/python_dictionaries.asp)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\r\n",
    "import time\r\n",
    "\r\n",
    "# Complétez le code ci-dessous de façon à récupérer le contenu de toutes les pages et \r\n",
    "# ensuite en enregistrez chaque entrée dans un tableau de dictionnaires. \r\n",
    "# N'oubliez pas de changer la valeur du paramètre \"numPage\" entre chaque tour de boucle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons maintenant un fichier de travail, vous pouvez maintenant d'exploiter ces données comme vu précédemment :\r\n",
    "- Nettoyage\r\n",
    "- Analyse\r\n",
    "- Graphiques\r\n",
    "\r\n",
    "# Pour aller plus loin\r\n",
    "Vous pouvez faire le même exercice avec des données immobilières, donnée que vous aurez récupérez sur le site https://www.avendrealouer.fr/"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c469c999aae2bf6d578d6881606ced546d1546d312a4811937df5b24665ee8bc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}