{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeautifulSoup\n",
    "\n",
    "BeautifulSoup est un bibliothèque de Python permettant de parser les documents HTML et XML, ainsi, il est possible d'extraire le contenu de page web selon des sélecteurs. La syntaxe des sélecteurs est la même que celle utilisée en javascript/jQuery/CSS.\n",
    "\n",
    "- Sélectionner tous les éléments ayant une classe : \".nom-de-classe\"\n",
    "  - `soup.select(\".nom-de-classe\")` \n",
    "  - `soup.find_all(attrs={\"class\": \".nom-de-classe\"})` \n",
    "- Sélectionner un élément ayant une classe contenue dans une autre : \".classe-1 .classe-2\"\n",
    "  - `soup.select_one(\".classe-1 .classe-2\")`\n",
    "  - `soup.find(\".classe-1\").find(\".classe-2\")`\n",
    "- Sélectionner un élement par son attribut : \"[src]\"\n",
    "  - `soup.select_one(\"[src]\")`\n",
    "  - `soup.find(attrs={\"src\": True})`\n",
    "- Sélectionner tous les élements ayant deux classes par son attribut : \".classe-1.classe-2\"\n",
    "  - `soup.select(\".classe-1.classe-2\")` \n",
    "  - `soup.find_all(attrs={\"class\": [\".classe-1\", \"classe-2\"]})` \n",
    "\n",
    "Notez un point très important : BeautifulSoup n'est pas très apté pour les scraper les données asynchrones, autrement dit, si votre site cible ajoute du contenu **après** le chargement initial de la page, il est fort probable que vous n'arrivez pas à le récupérer.\n",
    "\n",
    "BeautifulSoup est une librairie externe, première étape : l'installation et l'importation (inutile dans le cas de Google Colab).\n",
    "\n",
    "- [Voir documentation en anglais de BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pensez à commenter cette ligne après avoir importé beautifulsoup4 \n",
    "# pour éviter que conda vérifie s'il doit importer l'outil à chaque fois\n",
    "\n",
    "import sys\n",
    "# !conda install --yes --prefix {sys.prefix} beautifulsoup4\n",
    "# Plus d'infos ici sur la ligne précédente\n",
    "# https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/#How-to-use-Conda-from-the-Jupyter-Notebook\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour ne pas trop complexifier nos premiers pas avec BeautifulSoup, nous allons utiliser l'outil sans faire appel à des pages externes. Notre sujet d'expérimentation va être le fichier \"index.html\" présent dans le dossier de la ressource.\n",
    "\n",
    "Première étape, charger notre fichier \"index.html\" (déjà présent dans la ressource)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pour les utilisateurs de Google colab\n",
    "\n",
    "Petit apparté pour les utilisateurs de google colab. Pour charger un fichier local, il faudra rajouter les lignes de codes suivantes :\n",
    "\n",
    "```python\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "import io\n",
    "\n",
    "# Très important : le nom du fichier passé en paramètre de la fonction \"uploaded\" doit avoir le même nom que le fichier que vous avez uploadé sinon, vous aurez forcément une erreur\n",
    "page_web = io.BytesIO(uploaded['nom-du-fichier-uploader.ext'])\n",
    "```\n",
    "- **Ces lignes doivent être avant la manipulation d'un DataFrame et de préférence dans une cellule dédiée pour éviter d'uploader votre fichier à chaque fois**\n",
    "- **Vous ne pouvez pas importer de fichiers en navigation privée**\n",
    "\n",
    "- [Voir plus d'informations sur le chargement de fichiers externes avec Google colab - anglais](https://towardsdatascience.com/3-ways-to-load-csv-files-into-colab-7c14fcbdcb92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour les non-utilisateurs de google Colab \n",
    "chemin_du_fichier = \"index.html\"\n",
    "\n",
    "with open(chemin_du_fichier) as page_web:\n",
    "    soup = BeautifulSoup(page_web, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour les utilisateurs de Google Colab\n",
    "soup = BeautifulSoup(page_web, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voilà, notre fichier html est chargé et le contenu est chargé, sous forme d'objet, dans une variable nommée **soup**. Assurons-nous que tout fonctionne, pour ce faire, récupérons le titre de notre page (le contenu de la balise &lt;title>) et affichons-le grâce à la fonction \"print\" ou \"display\". Notre variable **\"soup\"** étant un objet, notre balise &lt;title> est une propriété de **\"soup\"** nommée **title**.\n",
    "\n",
    "Affichons donc la balise &lt;title> dans la cellule ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Ressource web-scraping</title>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Génial, ça fonctionne, en revanche, nous avons le contenu de votre balise title, mais entouré des balises. Pour n'obtenir que le texte d'une balise, il suffit de rajouter d'accéder à la propriété \".text\" de `soup.title` et là, nous n'obtiendrons que le texte. \n",
    "\n",
    "Prenez tout de fois en compte le fait que la propriété \"text\" affiche tout le texte de façon transverse, ainsi si vous accèdez à \".text\" au niveau de la balise \"body\" vous allez afficher tout le contenu textuel de la page web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichez ici le contenu textuel de la balise <title>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Désormais, on sait comment afficher le contenu textuel d'une balise. Allons un peu plus loin, essayons de récupérer une balise, disons la balise &lt;p> contenu dans la page. Même principe que précédemment, nous allons utiliser la variable **soup** mais cette fois-ci la propriété \".p\" car nous cherchons à récupérer la balise &lt;p>. Essayons !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichez ici la balise <p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Que remarquez-vous comparé à la structure de page index.html au niveau de la balise &lt;p> ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La propriété \".p\", c'est très bien. Toutefois, s'il y a plusieurs éléments, ça montre ses limites, la propriété \".p\" (ou tout autre balise), **ça ne retourne que la première occurence**, si on veut tout retourner, il faut utiliser la méthode \".select()\" ou \".find_all()\". Ces méthodes prennent en paramètre un sélecteur CSS. Les mêmes qu'on utilise pour écrire du CSS, ou sélectionner des éléments HTML en javascript.\n",
    "\n",
    "Pour commencer, on va faire simple : on va récupérer **toutes** nos balises &lt;p> avec la méthode \".select()\" ou \".find_all()\", ces deux méthodes sont à appliquer sur la variable `soup` et prennent tout deux, en premier paramètre, une balise HTML. Dans notre cas ça sera `soup.find_all('p')`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichez ici les balises <p> de la page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que le résultat est dans une liste, on ne pourra pas accéder directement la propriété \".text\" par exemple, vu que ce n'est pas une propriété de la liste, il faudra passer par une boucle pour ce faire. Essayez, complétez le code suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " for paragraphe in {votre liste contenant les paragraphes}:\n",
    "     # le texte de chaque paragraphe\n",
    "     display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \".select()\" ou \".find_all()\" ?\n",
    "Les deux font la même chose, prenez juste en compte le fait que \".select()\" est plus souple et puissant que \".find_all()\". \".select()\" permet notamment l'utilisation de sélecteurs CSS plus complexes comme les combinateurs. Par exemple `soup.select(\".paragraphe .txt-gras\")` écrire la même chose avec \".find_all()\" nécessiterait une variable pour stocker tous les \".paragraphe\" et un ensuite faire un \".find_all()\" sur \".txt-gras\". Le tout avec une syntaxe plus lourde comme suivi `soup.find_all(\"p\", {\"class\": \"paragraphe\"})`\n",
    "\n",
    "La méthode \".find_all()\" existe également en version \".find()\" qui retourne le premier résultat trouvé. De même pour la méthode `.select_one()`.\n",
    "\n",
    "Enfin, il est possible s'appliquer ces méthodes sur le résultat de ces mêmes méthodes.\n",
    "\n",
    "```python\n",
    "# Ici, je cherche, dans la première balise \"p\" ayant la classe \"paragraphe\" trouvée, tous les éléments ayant la classe \"txt-gras\"\n",
    "soup.find(\"p\", {\"class\": \"paragraphe\"}).select('.txt-gras')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"txt-gras\">\n",
       "             dans le but de le transformer pour permettre son utilisation dans un autre contexte, cette\n",
       "             technique peut Ãªtre trÃ¨s utile pour les data-journalistes.\n",
       "         </span>,\n",
       " <span class=\"txt-gras\">trÃ¨s</span>]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# La méthode .select() nous donne plus de possibilités en terme de sélecteurs. \n",
    "# Ces électeurs sont identiques à ceux utilisés en javascript/CSS/jQuery.\n",
    "soup.select(\".paragraphe .txt-gras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant à vous de jouer. Scrapez le site pour récupérer les données suivantes :\n",
    "- La valeur de l'attribut \"src\" des balises img contenue dans les articles\n",
    "  - select : .select(\"[attr='attr_name']\")\n",
    "  - find : .select(attrs={\"attr_name\"})\n",
    "- Le texte des entrées de la navigation\n",
    "- Le texte contenu dans le classe \"txt-gras\"\n",
    "\n",
    "Ces données doivent être placée dans des variables. Pour le premier cas, il faudra utiliser un tableau pour ajouter progressivement les données.\n",
    "\n",
    "- [Voir documentation sur les tableaux en Python - anglais](https://www.w3schools.com/python/python_arrays.asp)\n",
    "\n",
    "Dernier point : pour récuperer la valeur d'un attribut, il faut récupérer l'objet HTML et mettre entre crochets le nom de l'attribut dont on souhaite récupérer la valeur. Exemple : \n",
    "```soup.find(\"img\")[\"src\"]\n",
    "```\n",
    "**BeautifulSoup lèvera une erreur si vous écrivez entre les crochets un attribut qui n'existe pas.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Faites la pratique ici."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Super, on sait maintenant récupérer des données et les stocker, mais il nous manque quelque chose. Rappelons-nous un des objectifs du web-scraping \"Transformer des données de sites web en données structurées\". Jusqu'à présent, nous avons extrait des données mais nous ne les avons pas structurées. Autrement dit, nous ne pouvons pas faire grand-chose de nos extractions jusqu'à présent. Changeons ça.\n",
    "\n",
    "Pour structurer nos données, il faut que nos données soient sous la forme d'un structure ordonnées, il y a un modèle de données tout trouvé : le tableau de dictionnaires. Une fois transformé, ça ressemble à un fichier fait sous un tableur avec des colonnes prédéfinies.\n",
    "\n",
    "Pour rappel, un dictionnaire (appelé aussi \"tableau associatif\" ou \"objet\") est une structure où les index ne sont plus des nombres mais une chaîne de caractères. C'est à dire qu'un dictionnaire est un ensemble de clé:valeur, ça ressemble à ceci :\n",
    "```\n",
    "un_dictionnaire = {\n",
    "  \"marque\": \"Dell\",\n",
    "  \"modele\": \"XPS\",\n",
    "  \"annee\": 2014\n",
    "}\n",
    "```\n",
    "\n",
    "- [Voir documentation sur les dictionnaires en Python - anglais](https://www.w3schools.com/python/python_dictionaries.asp)\n",
    "\n",
    "Et ces dictionnaires, nous pouvons les mettre dans un tableau et donc réaliser un tableau de dictionnaires soit notre donnée structurée. Essayons de créer un tableau de dictionnaires avec la liste d'articles (\".article\") présente sur notre fichier index.html. Nos dictionnaires doivent contenir les clés suivantes \n",
    "- auteur : auteur de l'article\n",
    "- image : source de l'image\n",
    "- title : titre de l'article\n",
    "- date : date de l'article\n",
    "\n",
    "Encore une fois, il faudra stocker le tout dans une variable. N'oubliez pas qu'on veut un tableau, il faudra impérativement faire une boucle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Faites la pratique ici."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a notre donnée structurée, il nous manque une chose : un fichier exploitable et utilisable ailleurs sans faire toute ces commandes. Et on va réutiliser pandas pour ça. Première étape : l'importation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a importé pandas, avant de créer un fichier csv, il nous faut un DataFrame sinon, ça ne fonctionnera pas pour créer un fichier. Pour créer un DataFrame, il nous suffit juste de placer notre dictionnaire dans la méthode \".DataFrame()\" de pandas et ensuite sauvegarder le tout grâce à la méthode \".to_csv()\".\n",
    "\n",
    "Complétez le code suivant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({votre tableau de dictionnaire})\n",
    "\n",
    "df.to_csv(\"chemin-de-votre-fichier.csv\")\n",
    "# df.to_excel() pour créer un fichier excel\n",
    "# df.to_json() pour créer un fichier json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voilà, nous savons maintenant faire du scraper un site web et stocker les résultats dans un fichier, fichier qui pourra être utilisé plus tard pour faire de l'analyse, des modèles de données ou encore de la data-visualisation.\n",
    "\n",
    "Prochaine étape : la même chose mais sur le web, des sites distants. La suite se trouve dans le fichier request.ipynb.\n",
    "\n",
    "Avant de clôre cet exercice, sachez juste que si BeautifulSoup ne fonctionne pas, ne retourne pas ce que vous souhaitez, ceci signifie que les données sont asynchrones ou encore que le site nécessite du javascript pour fonctionner. Pour pallier à ce problème, il faudra utiliser en complément l'outil Selenium, il nous permettra de simuler un navigateur."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pour les utilisateurs de Google colab\n",
    "\n",
    "Petit apparté pour les utilisateurs de google colab. Pour utiliser la méthode `to_csv()` (ou autre), il faudra rajouter quelques lignes de codes supplémentaires pour pouvoir **télécharger** un fichier, les voici.\n",
    "\n",
    "```python\n",
    "from google.colab import files\n",
    "nom_fichier = \"chemin-de-votre-fichier.csv\" # Le même que définit plus haut\n",
    "files.download(nom_fichier) \n",
    "```\n",
    "\n",
    "- [Voir plus d'informations sur l'enregistrement de fichiers depuis google colab](https://colab.research.google.com/notebooks/io.ipynb#scrollTo=hauvGV4hV-Mh&line=4&uniqifier=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c469c999aae2bf6d578d6881606ced546d1546d312a4811937df5b24665ee8bc"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit (conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
